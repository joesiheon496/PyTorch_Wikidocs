```python
import torch
import torch.nn.functional as F
torch.manual_seed(1)
```

## 1. 파이토치로 소프트맥스의 비용 함수 구현하기 (로우-레벨)
```python
z = torch.FloatTensor([1, 2, 3])

```
```python
hypothesis = F.softmax(z,dim = 0)
```
```python
z = torch.rand(3,5, requires_grad = True)
```

- 각 샘플에 대해서 소프트맥스 함수를 적용하여야 하므로 두번째 차원에 대해서 소프트맥스 함수를 적용한다는 의미에서 dim=1을 써줍니다.


```python
hypothesis = F.softmax(z,dim=1)
print(hypothesis)
```

- 이제 각 행의 원소들의 합은 1이 되는 텐서로 변환되었습니다. 소프트맥스 함수의 출력값은 결국 예측값
- 이제 각 샘플에 대해서 임의의 레이블을 만듭니다.

```python
y = torch.randint(5,(3,)).long() # long (long integers) : int 보다 범위가 큰 정수
print(y)

tensor([0, 2, 1])
```
- 이제 각 레이블에 대해서 원-핫 인코딩을 수행합니다.

```python
y_one_hot = torch.zeros_like(hypothesis)
y_one_hot.scatter_(1,y.unsqueeze(1),1) # unsqueeze함수는 squeeze함수의 반대로 1인 차원을 생성하는 함수이다. 그래서 어느 차원에 1인 차원을 생성할 지 꼭 지정해주어야한다.
# squeeze squeeze함수는 차원이 1인 차원을 제거해준다. 따로 차원을 설정하지 않으면 1인 차원을 모두 제거한다. 그리고 차원을 설정해주면 그 차원만 제거한다.

```
```python
import torch

x = torch.rand(3, 1, 20, 128)
x = x.squeeze() #[3, 1, 20, 128] -> [3, 20, 128]

import torch

x = torch.rand(1, 1, 20, 128)
x = x.squeeze() # [1, 1, 20, 128] -> [20, 128]
# 주의할 점은 생각치도 못하게 batch가 1일 때 batch차원도 없애버리는 불상사가 발생할 수있다. 그래서 validation단계에서 오류가 날 수 있기 때문에 주의해서 사용해야 한다.
x2 = torch.rand(1, 1, 20, 128)
x2 = x2.squeeze(dim=1) # [1, 1, 20, 128] -> [1, 20, 128]
```
```python
import torch

x = torch.rand(3, 20, 128)
x = x.unsqueeze(dim=1) #[3, 20, 128] -> [3, 1, 20, 128]
```

- torch.zeros_like(hypothesis)를 통해 모든 원소가 0의 값을 가진 3 × 5 텐서를 만듭니다.
- 두번째 줄을 해석해봅시다. y.unsqueeze(1)를 하면 (3,)의 크기를 가졌던 y 텐서는 (3 × 1) 텐서가 됩니다.

```python
print(y.unsqueeze(1))
```
```python
tensor([0, 2, 1])
# ->
tensor([[0],
        [2],
        [1]])
```
- 그리고 scatter의 첫번째 인자로 dim=1에 대해서 수행하라고 알려주고, 세번째 인자에 숫자 1을 넣어주므로서 두번째 인자인 y_unsqeeze(1)이 알려주는 위치에 숫자 1을 넣도록 합니다.
- 앞서 텐서 조작하기 2챕터에서 연산 뒤에 _를 붙이면 In-place Operation (덮어쓰기 연산)임을 배운 바 있습니다. 이에 따라서 y_one_hot의 최종 결과는 결국 아래와 같습니다.

```python
print(y_one_hot)
tensor([[1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.]])
```
$$cost(W) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k}y_{j}^{(i)}\ log(p_{j}^{(i)})$$

$$cost(W) = \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k}y_{j}^{(i)}\ × (-log(p_{j}^{(i)}))$$

- $\sum_{j=1}^{k}$는 sum(dim=1)으로 구현
- $\frac{1}{n} \sum_{i=1}^{n}$는 mean()으로 구현합니다.
```python
cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()
print(cost)
```
## 2. 파이토치로 소프트맥스의 비용 함수 구현하기 (하이-레벨)
### 1. F.softmax() + torch.log() = F.log_softmax()
```python
torch.log(F.softmax(z,dim=1))

tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],
        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],
        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)
```
- 파이토치에서는 두 개의 함수를 결합한 F.log_softmax()라는 도구를 제공합니다.

```python
F.log_softmax(z, dim=1)
tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],
        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],
        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogSoftmaxBackward>)
```

### 2. F.log_softmax() + F.nll_loss() = F.cross_entropy()
```python
(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()
# ->
(y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()
```
- 이를 더 간단하게 하면 다음과 같습니다. F.nll_loss()를 사용할 때는 원-핫 벡터를 넣을 필요없이 바로 실제값을 인자로 사용합니다.

```python
# High level
# 세번째 수식
F.nll_loss(F.log_softmax(z, dim=1), y)
```
-  이를 더 간단하게 하면 다음과 같이 사용할 수 있습니다. F.cross_entropy()는 F.log_softmax()와 F.nll_loss()를 포함
```python
# 네번째 수식
F.cross_entropy(z, y)
```













