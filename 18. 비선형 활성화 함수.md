# 시그모이드와 기울기 소실

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/78461381-cd44-44d9-92cf-039925be04f4)

```python
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
  return 1/(1+np.exp(-x)
x = np.range(-5,5,0.01)
y = sigmoid(x)

plt.plot(x,y)
plt.plot([0,0],[1,0,0,0],":")
plt.title("sigmoid")
plt.show()
```

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/d4ba1229-589c-4bb4-b711-c34876588763)


![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/6875417e-46b1-4cbc-bbb0-afab3ac5909d)

* 출력값이 0또는 1에 가까워지면 그래프의 기울기가 완만해진다

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/4c6675d2-33c9-42dd-a6b1-f8702f84cb8c)

* 이러한 문제를 **기울기 소실 (Vanishing Gradient)라고한다**


![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/6a2a97cc-fdb1-4b89-bb79-cfe2dd2ab875)

## 하이퍼볼릭탄젠트 함수 (Hyperbolic tangent function)
```python
x = np.arange(-5.0,5.0,0.1)
y = np.tanh(x)

plt.plot(x,y)
plt.plot([0.0],[1.0,-1.0],':')
plt.axhline(y=0, color='orange', linestyle='--')
plt.title('Tanh Function')
plt.show()
```

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/6ac66164-6a40-4df3-a081-d97deb29f3b6)

## ReLU 함수

```python
def relu(x):
  return np.max(0,x)

x = np.range(-5.0,5.0,0.1)
y = relu(x)

plt.plot(x,y)
plt.plot([0,0],[5.0,0.0],':')
plt.title('ReLU FUnction')
plt.show()

```

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/58edf403-0ee3-4e62-8a55-a4bee611b6ac)


## 리키 렐루(Leaky ReLU)

```python
a = 0.1
def leaky_relu(x):
  return np.maximum(a*x,x)
  
x = np.arange(-5.0,5.0,0.1)
y = leaky_relu(x)

plt.plot(x,y)
plt.plot([0,0],[5.0,0.0],':')
plt.title('leaky relu')
plt.show()
```


## 소프트 맥스
```python
x = np.arange(-5.0,5.0,0.1)
y = np.exp(x) / np.sum(exp(x))

plt.plot(x,y)
plt.title('softmax function')
plt.show()
```

## 출력층의 활성화 함수와 오차 함수의 관계
|문제|활성화 함수|비용함수|
|-------|--------|----------|
|이진분류|시그모이드|nn.BCELoss()|
|다중클래스분류|소프트맥스|nn.CrossEntropyLoss()|
|회귀|없음|MSE|
