# 소프트맥스 회귀(Softmax Regression) 이해
- 소프트맥스 회귀를 통해 3개 이상의 선택지 중에서 1개를 고르는 다중 클래스 분류(Multi-Class Classification)를 실습

## 다중 클래스 분류(multi-class classification)
- 세 개 이상의 답 중 하나를 고르는 문제를 다중 클래스 분류(Multi-class Classification)라고 합니다.

<img width="708" alt="image" src="https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/f1c87823-9f22-4452-879b-1f044f53d7e8">

## 로지스틱 회귀
- 로지스틱 회귀에서 시그모이드 함수는 예측값을 0과 1 사이의 값으로 만듭니다. 예를 들어 스팸 메일 분류기를 로지스틱 회귀로 구현하였을 때, 출력이 0.75이라면 이는 이메일이 스팸일 확률이 75%라는 의미가 됩니다. 반대로, 스팸 메일이 아닐 확률은 25%가 됩니다. 이 두 확률의 총 합은 1

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/80ae0e59-1135-442b-aaca-0c90e7db1986)

$$H(X) = sigmoid(WX + B)$$

## 소프트맥스 회귀
- 소프트맥스 회귀는 확률의 총 합이 1이 되는 이 아이디어를 다중 클래스 분류 문제에 적용합니다. 소프트맥스 회귀는 각 클래스. 즉, 각 선택지마다 소수 확률을 할당합니다. 이때 총 확률의 합은 1이 되어야 합니다. 이렇게 되면 각 선택지가 정답일 확률로 표현

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/d4af0d4e-c8c5-4724-8a3b-3f89ae9e7fe2)

- 붓꽃 품종 분류하기 문제 등과 같이 선택지의 개수가 3개일때, 3차원 벡터가 어떤 함수 ?를 지나 원소의 총 합이 1이 되도록 원소들의 값이 변환되는 모습

$$H(X) = softmax(WX + B)$$

# 소프트맥스 함수(Softmax function)
- k차원의 벡터에서 i번째 원소를 $z_i$, i번째 클래스가 정답일 확률을 $p_i$로 나타낸다고 하였을 때 소프트맥스 함수 $p_i$는 다음과 같다

$$p_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{k} e^{z_{j}}}\ \ for\ i=1, 2, ... k$$

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/62370a27-9b2b-4685-9cd6-b051032701eb)

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/441341cf-67e2-46a5-8243-1353e6155811)

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/1d4a9518-1fa2-4ce7-bab4-d622a6208534)

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/046edd88-7e68-424f-b31d-11fec9432697)

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/8b453f4d-38bb-4ae2-bd71-39455c56df98)

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/7f98399c-b77a-4c59-bd6e-b427439bf7bd)

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/0f788e86-6acd-45f0-a25c-b807dd79c483)

$$X=
\left(
    \begin{array}{c}
      5.1\ 3.5\ 1.4\ 0.2\ \\
      4.9\ 3.0\ 1.4\ 0.2\ \\
      5.8\ 2.6\ 4.0\ 1.2\ \\
      6.7\ 3.0\ 5.2\ 2.3\ \\
      5.6\ 2.8\ 4.9\ 2.0\ \\
    \end{array}
  \right)$$
  
$$X=\left(
    \begin{array}{c}
      x_{11}\ x_{12}\ x_{13}\ x_{14}\ \\
      x_{21}\ x_{22}\ x_{23}\ x_{24}\ \\
      x_{31}\ x_{32}\ x_{33}\ x_{34}\ \\
      x_{41}\ x_{42}\ x_{43}\ x_{44}\ \\
      x_{51}\ x_{52}\ x_{53}\ x_{54}\ \\
    \end{array}
  \right)$$
  
$$\hat{Y}=\left(
    \begin{array}{c}
      y_{11}\ y_{12}\ y_{13}\ \\
      y_{21}\ y_{22}\ y_{23}\ \\
      y_{31}\ y_{32}\ y_{33}\ \\
      y_{41}\ y_{42}\ y_{43}\ \\
      y_{51}\ y_{52}\ y_{53}\ \\
    \end{array}
  \right)$$
  
  $$W=\left(
    \begin{array}{c}
      w_{11}\ w_{12}\ w_{13}\ \\
      w_{21}\ w_{22}\ w_{23}\ \\
      w_{31}\ w_{32}\ w_{33}\ \\
      w_{41}\ w_{42}\ w_{43}\ \\
    \end{array}
  \right)$$
  
  
$$B=\left(
    \begin{array}{c}
      b_{1}\ b_{2}\ b_{3}\\
      b_{1}\ b_{2}\ b_{3}\\
      b_{1}\ b_{2}\ b_{3}\\
      b_{1}\ b_{2}\ b_{3}\\
      b_{1}\ b_{2}\ b_{3}\\
    \end{array}
  \right)$$
  
$$\hat{Y} = softmax(XW + B)$$

$$\left(
    \begin{array}{c}
      y_{11}\ y_{12}\ y_{13}\ \\
      y_{21}\ y_{22}\ y_{23}\ \\
      y_{31}\ y_{32}\ y_{33}\ \\
      y_{41}\ y_{42}\ y_{43}\ \\
      y_{51}\ y_{52}\ y_{53}\ \\
    \end{array}
  \right)
=
softmax\left(
\left(
    \begin{array}{c}
      x_{11}\ x_{12}\ x_{13}\ x_{14}\ \\
      x_{21}\ x_{22}\ x_{23}\ x_{24}\ \\
      x_{31}\ x_{32}\ x_{33}\ x_{34}\ \\
      x_{41}\ x_{42}\ x_{43}\ x_{44}\ \\
      x_{51}\ x_{52}\ x_{53}\ x_{54}\ \\
    \end{array}
  \right)
\left(
    \begin{array}{c}
      w_{11}\ w_{12}\ w_{13}\ \\
      w_{21}\ w_{22}\ w_{23}\ \\
      w_{31}\ w_{32}\ w_{33}\ \\
      w_{41}\ w_{42}\ w_{43}\ \\
    \end{array}
  \right)
+
\left(
    \begin{array}{c}
      b_{1}\ b_{2}\ b_{3}\\
      b_{1}\ b_{2}\ b_{3}\\
      b_{1}\ b_{2}\ b_{3}\\
      b_{1}\ b_{2}\ b_{3}\\
      b_{1}\ b_{2}\ b_{3}\\
    \end{array}
  \right)
\right)$$

# 비용 함수(cost function)
- 소프트맥스 회귀에서는 비용 함수로 크로스 엔트로피 함수를 사용
- 소프트맥스 회귀에서의 크로스 엔트로피 함수뿐만 아니라, 다양한 표기 방법에 대해서 이해
## 1) 크로스 엔트로피 함수
$$cost(W) = -\sum_{j=1}^{k}y_{j}\ log(p_{j})$$

$$cost(W) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k}y_{j}^{(i)}\ log(p_{j}^{(i)})$$

## 이진 분류에서의 크로스 엔트로피 함수
$$cost(W) = -(y\ logH(X) + (1-y)\ log(1-H(X)))$$

$$-(y_{1}\ log(p_{1})+y_{2}\ log(p_{2}))$$

$$-(\sum_{i=1}^{2}y_{i}\ log\ p_{i})$$

$$-(\sum_{i=1}^{k}y_{i}\ log\ p_{i})$$

$$cost(W) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k}y_{j}^{(i)}\ log(p_{j}^{(i)}) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})]$$





