image size : 28 $times$ 28 = 784 feature

```python
for X,Y in data_loader:
  X = X.view(-1,28*28)
```
- 위의 코드에서 X는 for문에서 호출될 때는 (배치 크기 × 1 × 28 × 28)의 크기를 가지지만, view를 통해서 (배치 크기 × 784)의 크기로 변환됩니다.

# 2. 토치비전(torchvision) 소개하기
- 이미지 처리
### 자연어 처리는 토치텍스트(torchtext)

# 3. 분류기 구현을 위한 사전 설정
```python
import torch
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch.nn as nn
import matplotlib.pyplot as plt
import random
```

```python
USE_CUDA = torch.cuda.is_available()
device = torch.device("cuda" if USE_CUDA else "cpu")
print(f"다음 기기로 학습합니다:{device}")
```
```python
random.seed(777)
torch.manual_seed(777)
if device = "cuda":
  torch.cuda.manual_seed_all(777)
```
```python
training_epochs = 15
batch_size = 100
```
```python
mnist_train = dsets.MNIST(root="MNIST_data/",
                          train = True,
                          transform = transforms.ToTenser(),
                          download = True)
mnist_test = dsets.MNIST(root="MNIST_data/",
                          train = False,
                          transform = transforms.ToTenser(),
                          download = True)
```
- 첫번째 인자 root는 MNIST 데이터를 다운로드 받을 경로
- 두번째 인자 train은 인자로 True를 주면, MNIST의 훈련 데이터를 리턴받으며 False를 주면 테스트 데이터를 리턴
- 세번째 인자 transform은 현재 데이터를 파이토치 텐서로 변환
- 네번째 인자 download는 해당 경로에 MNIST 데이터가 없다면 다운로드 받겠다는 의미
- 미니 배치와 데이터로드 챕터에서 학습했던 데이터로더(DataLoader)를 사용
```python
data_loader = DataLoader(dataset = mnist_train,
                          batch_size = batch_size,
                          shuffle = True,
                          drop_last = True)
```
- 첫번째 인자인 dataset은 로드할 대상
- 두번째 인자인 batch_size는 배치 크기
- shuffle은 매 에포크마다 미니 배치를 셔플할 것인지의 여부
- drop_last는 마지막 배치를 버릴 것인지를 의미
  - drop_last를 하는 이유를 이해하기 위해서 1,000개의 데이터가 있다고 했을 때, 배치 크기가 128이라고 해봅시다. 1,000을 128로 나누면 총 7개가 나오고 나머지로 104개가 남습니다. 이때 104개를 마지막 배치로 한다고 하였을 때 128개를 충족하지 못하였으므로 104개를 그냥 버릴 수도 있습니다. 이때 마지막 배치를 버리려면 drop_last=True를 해주면 됩니다.
  - 다른 미니 배치보다 개수가 적은 마지막 배치를 경사 하강법에 사용하여 마지막 배치가 상대적으로 과대 평가되는 현상을 막아줍니다.

- input_dim은 784이고, output_dim은 10
```python
linear = nn.Linear(784, 10, bias=True).to(device)
```
- to() 함수는 연산을 어디서 수행할지를 정합니다.
- GPU를 사용하려면 to('cuda')를 해 줄 필요가 있습니다.
- 아무것도 지정하지 않은 경우에는 CPU 연산이라고 보면 됩니다.
- bias는 편향 b를 사용할 것인지를 나타냅니다. 기본값은 True이므로 굳이 할 필요는 없지만 명시적으로 True를 해주었습니다.

```python
criterion = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)
```

```python
for epoch in range(training_epochs):
  avg_cost = 0
  total_batch = len(data_loader)
  for X,Y in data_loader:
    X = X.view(-1.28*28).to(device)
    Y = Y.to(device)
    
    optimizer.zero_grad()
    criterion.backward()
    optimizer.step()
    
    avg_cost += cost/total_batch
  print("{epoch+1}, {avg_cost}
print(f"Learning finished")
```
### TEst
```python
with torch.no_grad():
  X_test = mnist_test.test_data.view(-1,28*28).float().to(device)
  Y_test = mnist_test.test_labels.to(device)
  
  prediction = linear(X_test)
  correct_prediction = torch.argmax(prediction,1) == Y_test
  accuracy = correct_prediction.float().mean()
  print(f"{accuracy.item()}")
  
  r = random.randint(0,len(mnist_test)-1)
  X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)
  Y_single_data = mnist_test.test_labels[r:r+1].to(device)

  print(f"label:{Y_single_data.item()}"
  single_prediction = linear(X_single_data)
  print(f"prediction:{torch.argmax(single_prediction,1).item()}")
```

```python

```










