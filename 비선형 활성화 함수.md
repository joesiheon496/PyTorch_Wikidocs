```python
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
  return 1/(1+np.exp(-x)
x = np.range(-5,5,0.01)
y = sigmoid(x)

plt.plot(x,y)
plt.plot([0,0],[1,0,0,0],":")
plt.title("sigmoid")
plt.show()
```

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/d4ba1229-589c-4bb4-b711-c34876588763)


![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/6875417e-46b1-4cbc-bbb0-afab3ac5909d)

* 출력값이 0또는 1에 가까워지면 그래프의 기울기가 완만해진다

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/4c6675d2-33c9-42dd-a6b1-f8702f84cb8c)

* 이러한 문제를 **기울기 소실 (Vanishing Gradient)라고한다**


![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/6a2a97cc-fdb1-4b89-bb79-cfe2dd2ab875)

![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/f84dc7a8-6428-4cea-a8b1-6315902e2824)
