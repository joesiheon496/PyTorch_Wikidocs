# 미니배치(mini-batch)
- 배치 크기는 보통 2의 제곱수를 사용합니다. ex) 2, 4, 8, 16, 32, 64... 그 이유는 CPU와 GPU의 메모리가 2의 배수이므로 배치크기가 2의 제곱수일 경우에 데이터 송수신의 효율을 높일 수 있다

<img width="400" alt="image" src="https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/0a311611-a341-45cd-bd6a-e9c6687cf9be">

# 이터레이션(iteration)
- 이터레이션은 한 번의 에포크 내에서 이루어지는 매개변수인 가중치 W와 b의 업데이트 횟수
<img width="324" alt="image" src="https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/bf981314-bf76-4da9-8c83-e736e2c18276">
- 2,000일 때 배치 크기를 200으로 한다면 이터레이션의 수는 총 10개
- 한 번의 에포크 당 매개변수 업데이트가 10번 이루어짐을 의미

# 데이터 로드하기(Data Loader)
- 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행
- 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
```

## datasaet
```python
x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  90], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])
```

```python
dataset = TensorDataset(x_train,y_train)
```
```python
dataloader = DataLoader(dataset, batch_size=2,shuffle=True)
```

```python
model = nn.Linear(3,1)
optimizer = nn.optim.SGD(model.parameters(),lr= 1e-5)
```

```python
nb_epochs = 20000
for epoch in range(nb_epochs+1):
  for batch_idx, sample in enumerate(dataloader):
    x_train,y_train = sample
    prediction = model(x_train)
    
    cost = F.mse_loss(prediction,y_train)
    
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    print(f"")
```
## 값 뽑아보는법
```python
new_var = torch.FloatTensor([[24,3,53]])
pred_y = model(new_var)

print(f"{pred_y}")
```
























