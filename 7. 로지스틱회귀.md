- 둘 중 하나를 결정하는 문제를 이진 분류(Binary Classification)라고 합니다. 그리고 이진 분류를 풀기 위한 대표적인 알고리즘으로 로지스틱 회귀(Logistic Regression)가 있습니다.

- 로지스틱 회귀는 알고리즘의 이름은 회귀이지만 실제로는 분류(Classification) 작업에 사용할 수 있습니다.

$H(x) = f(Wx + b)$



# 시그모이드(Sigmoid function)

$$H(x) = sigmoid(Wx + b) = \frac{1}{1 + e^{-(Wx + b)}} = σ(Wx + b)$$

```python
%matplotlib inline
import numpy as np
import matpotlib.pyplot as plt

# 시그모이드 식
def sigmoid(x):
  return 1/(1+np.exp(-x))
```
## W가 1이고 b가 0인 그래프
```python
x = np.arange(-5,0,5.0,0.1)
y = sigmoid(x)

plt.plot(x,y,'g')
plt.plot([0,0],[1.0,0.0],':')# 가운데 점선 추가
plt.title('sigmoid function')
plt.show()
```
<img width="383" alt="image" src="https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/49a2888a-d0ee-4829-b336-c135a4fcb317">

## W값의 변화에 따른 경사도의 변화

```python
x = np.arange(-5.0,5.0,0.1)
y1 = sigmoid(0.5*x)
y2 = sigmoid(x)
y3 = sigmoid(2*x)

plt.plot(x,y1,'r',linestyle = '--')
plt.plot(x,y2,'g')
plt.plot(x,y3,'b',linestyle='--')
plt.plot([0,0],[1.0,0.0],':')
plt.title("sigmoid")
plt.show()
```
<img width="390" alt="image" src="https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/59f2b2d1-0688-4d40-a374-93865f8e3f94">

## 시그모이드 함수르 이용한 분류
- 시그모이드 함수의 출력값은 0과 1 사이의 값을 가지는데 이 특성을 이용하여 분류 작업에 사용
- 예를 들어 임계값을 0.5라고 정해보겠습니다. 출력값이 0.5 이상이면 1(True), 0.5이하면 0(False)으로 판단

## 비용함수(Cost function)
$H(x) = sigmoid(Wx + b)$ 
![image](https://github.com/joesiheon496/PyTorch_Wikidocs/assets/56191064/4cd62e98-5aee-4661-990b-3df3f0c153ae)
$$\text{if } y=1 → \text{cost}\left( H(x), y \right) = -\log(H(x))$$
$$\text{if } y=0 → \text{cost}\left( H(x), y \right) = -\log(1-H(x))$$

$$\text{cost}\left( H(x), y \right) = -[ylogH(x) + (1-y)log(1-H(x))]$$

- 선형 회귀에서는 모든 오차의 평균을 구해 평균 제곱 오차를 사용했었습니다. 마찬가지로 여기에서도 모든 오차의 평균을 구합니다.

$$cost(W) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]$$

- 위 비용 함수에 대해서 경사 하강법을 수행하면서 최적의 가중치 W를 찾아갑니다.
$$W := W - α\frac{∂}{∂W}cost(W)$$

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
torch.manual_seed(1)
```
```python
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]] # 6*2
y_data = [[0], [0], [0], [1], [1], [1]] # 6 * 1
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)
```
```python
W = torch.zeros((2,1),requires_grad = True) # 2*1
b = torch.zeros(1,requires_grad = True)

hypothesis = 1 / torch.exp(-(x_train.matmul(W)+b)))
print(hypothesis)
```
```python
tensor([[0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000]], grad_fn=<MulBackward>)
```

```python
hypothesis = torch.sigmoid(x_train.matmul(W)+b)
```

$$cost(W) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]$$


```python
losses = -(y_train * torch.log(hypothesis)+(1 - y_train)*torch.log(1-hypothesis))
cost = losses.mean()

# 이것은 정리되어있음
F.binary_cross_entropy(hypothesis, y_train)
```

